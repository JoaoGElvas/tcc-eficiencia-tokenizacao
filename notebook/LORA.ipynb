{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Lista de pastas criadas nos passos anteriores\n",
    "pastas_lixo = [\n",
    "    \"./debug_out\",\n",
    "    \"./debug_low_ram\",\n",
    "    \"./gemma_mwe_finetuned\",\n",
    "    \"./gemma_a100_finetuned\",\n",
    "    \"./sample_data\" # Pasta padrão do Colab\n",
    "]\n",
    "\n",
    "total_liberado = 0\n",
    "\n",
    "for pasta in pastas_lixo:\n",
    "    if os.path.exists(pasta):\n",
    "        try:\n",
    "            shutil.rmtree(pasta)\n",
    "            print(f\"Apagada: {pasta}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao apagar {pasta}: {e}\")\n",
    "    else:\n",
    "        print(f\"Já não existia: {pasta}\")\n",
    "\n",
    "print(\"\\nLimpeza concluída! Pode rodar o treino.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92359b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- 1. PREPARAÇÃO DO AMBIENTE ---\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"INICIANDO EXPERIMENTO...\")\n",
    "\n",
    "# --- 2. DADOS E SEPARAÇÃO ---\n",
    "print(\"Carregando e Dividindo Dataset...\")\n",
    "dataset = load_dataset(\"TucanoBR/GigaVerbo-Text-Filter\", name=\"default\", split=\"train\")\n",
    "\n",
    "# Seleciona 40k amostras\n",
    "full_dataset = dataset.select(range(40000))\n",
    "\n",
    "# Divisão 90% Treino / 10% Teste\n",
    "dataset_split = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]\n",
    "\n",
    "print(f\"Treino: {len(train_dataset)} amostras | Teste: {len(eval_dataset)} amostras\")\n",
    "\n",
    "# --- 3. TREINAMENTO DO MWE ---\n",
    "print(\"Treinando detector de MWE...\")\n",
    "stream = (row['text'].lower().split() for row in train_dataset)\n",
    "phrases = Phrases(stream, min_count=10, threshold=15.0)\n",
    "bigram_phraser = Phraser(phrases)\n",
    "\n",
    "# Filtro Top 20k\n",
    "all_mwe = list(phrases.export_phrases().items())\n",
    "all_mwe_sorted = sorted(all_mwe, key=lambda x: x[1], reverse=True)\n",
    "new_tokens = [mwe[0].decode('utf-8') if isinstance(mwe[0], bytes) else mwe[0] for mwe in all_mwe_sorted[:20000]]\n",
    "print(f\"Vocabulário Novo: {len(new_tokens)} tokens.\")\n",
    "\n",
    "# --- 4. PRÉ-PROCESSAMENTO (MAP) ---\n",
    "print(\"Aplicando MWE nos conjuntos...\")\n",
    "\n",
    "def apply_mwe_to_row(example):\n",
    "    tokens = example['text'].lower().split()\n",
    "    new_tokens_list = bigram_phraser[tokens]\n",
    "    return {\"text\": \" \".join(new_tokens_list)}\n",
    "\n",
    "train_dataset = train_dataset.map(apply_mwe_to_row, num_proc=4)\n",
    "eval_dataset = eval_dataset.map(apply_mwe_to_row, num_proc=4)\n",
    "\n",
    "# --- 5. MODELO & INJEÇÃO ---\n",
    "print(\"Carregando Gemma 2B (A100/bf16)...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "MODEL_ID = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, quantization_config=bnb_config, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Injetando Vocabulário...\")\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# --- 6. LORA ---\n",
    "peft_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    modules_to_save=[\"embed_tokens\", \"lm_head\"]\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# --- 7. TREINAMENTO ---\n",
    "print(\"Configurando Treinador...\")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./gemma_robust_final\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    max_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    dataset_text_field=\"text\"\n",
    ")\n",
    "\n",
    "# Injeção Manual\n",
    "sft_config.max_seq_length = 512\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Rodando Treino Final (Monitore o 'Eval Loss')...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- 8. SALVAR ---\n",
    "print(\"Salvando o Modelo Campeão...\")\n",
    "FINAL_PATH = \"gemma-2b-robust-final\"\n",
    "trainer.model.save_pretrained(FINAL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_PATH)\n",
    "\n",
    "print(f\"\\nEXPERIMENTO CONCLUÍDO! Modelo salvo em: {FINAL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"CARREGANDO O MODELO CAMPEÃO...\")\n",
    "\n",
    "# --- CAMINHOS ---\n",
    "MODEL_ID = \"google/gemma-2b-it\"\n",
    "# Atenção: Usando o caminho do treino robusto agora\n",
    "ADAPTER_PATH = \"gemma-2b-robust-final\"\n",
    "\n",
    "# --- CARREGAR ---\n",
    "# 1. Tokenizer (com os 20k tokens novos)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)\n",
    "\n",
    "# 2. Modelo Base\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 3. Resize e Acoplagem do LoRA\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "\n",
    "# --- TESTE DE GERAÇÃO ---\n",
    "print(\"\\nENTREVISTANDO O MODELO...\")\n",
    "\n",
    "def testar_modelo(prompt):\n",
    "    # O prompt entra puro, o tokenizer já sabe que 'taxa_de_juros' é um token só\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=60,\n",
    "            do_sample=True,\n",
    "            temperature=0.6, # Temperatura baixa para ser mais racional\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompts_finais = [\n",
    "    \"A taxa_de_juros elevada pode causar\",\n",
    "    \"O governo_federal deve investir em\",\n",
    "    \"A inteligencia_artificial mudou o mundo porque\",\n",
    "    \"O rio_de_janeiro enfrenta problemas com\",\n",
    "    \"Durante o fim_de_semana as pessoas gostam de\"\n",
    "]\n",
    "\n",
    "for p in prompts_finais:\n",
    "    print(f\"{p}...\")\n",
    "    res = testar_modelo(p)\n",
    "    print(f\"{res}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
