{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd14026",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Celula 0: Atualizando as bibliotecas para as versoes mais recentes\n",
    "!pip install --upgrade datasets transformers accelerate bitsandbytes\n",
    "\n",
    "# Celula Principal: Imports, Configuracoes e Execucao\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- 1. CONFIGURACOES DO EXPERIMENTO ---\n",
    "NUM_SAMPLES = 500\n",
    "MAX_TEXT_LENGTH_WORDS = 4000  # Limite maximo de palavras por texto\n",
    "\n",
    "# Configuracao para carregar os datasets corretamente\n",
    "DATASETS_CONFIG = {\n",
    "    \"pt\": {\"name\": \"TucanoBR/GigaVerbo-Text-Filter\", \"config_name\": \"default\", \"text_column\": \"text\"},\n",
    "    \"en\": {\"name\": \"ag_news\", \"text_column\": \"text\"},\n",
    "}\n",
    "\n",
    "STATIC_TOKENIZERS = [\"xlm-roberta-base\", \"google/mt5-base\"]\n",
    "DYNAMIC_MODEL = \"google/gemma-2b-it\"\n",
    "MWE_DICT_PT = {\"processamento de linguagem natural\": \"PLN\"}\n",
    "\n",
    "# --- 2. CLASSES DE PRE-PROCESSAMENTO ---\n",
    "class BasePreprocessor:\n",
    "    def apply(self, text: str) -> str:\n",
    "        raise NotImplementedError\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "class BaselinePreprocessor(BasePreprocessor):\n",
    "    def apply(self, text: str) -> str:\n",
    "        return text\n",
    "\n",
    "class RegexPreprocessor(BasePreprocessor):\n",
    "    def apply(self, text: str) -> str:\n",
    "        processed_text = text.lower()\n",
    "        processed_text = re.sub(r'([.,!?;])', r' \\1 ', processed_text)\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "        return processed_text\n",
    "\n",
    "class MWEPreprocessor(BasePreprocessor):\n",
    "    def __init__(self, mwe_dict: dict):\n",
    "        self.mwe_dict = sorted(mwe_dict.keys(), key=len, reverse=True)\n",
    "        self.mwe_map = {mwe: mwe.replace(' ', '_') for mwe in self.mwe_dict}\n",
    "    def apply(self, text: str) -> str:\n",
    "        processed_text = text.lower()\n",
    "        for mwe in self.mwe_dict:\n",
    "            processed_text = processed_text.replace(mwe, self.mwe_map[mwe])\n",
    "        return processed_text\n",
    "\n",
    "# --- 3. CARREGANDO O MODELO DINAMICO NA GPU ---\n",
    "print(\"Carregando o modelo dinamico na GPU (pode demorar alguns minutos)...\")\n",
    "dynamic_tokenizer = AutoTokenizer.from_pretrained(DYNAMIC_MODEL)\n",
    "dynamic_model = AutoModelForCausalLM.from_pretrained(\n",
    "    DYNAMIC_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "print(\"Modelo dinamico carregado com sucesso!\")\n",
    "\n",
    "# --- 4. FUNCOES DE COLETA DE METRICAS ---\n",
    "def get_static_metrics(text: str, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    num_tokens = len(tokens)\n",
    "    num_words = len(text.split())\n",
    "    fertility = num_tokens / num_words if num_words > 0 else 0\n",
    "    return num_tokens, fertility\n",
    "\n",
    "def get_dynamic_metrics(text: str, tokenizer, model):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time\n",
    "    except Exception as e:\n",
    "        print(f\"[Erro na geracao] {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 5. EXECUCAO PRINCIPAL ---\n",
    "preprocessors = [\n",
    "    BaselinePreprocessor(),\n",
    "    RegexPreprocessor(),\n",
    "    MWEPreprocessor(mwe_dict=MWE_DICT_PT)\n",
    "]\n",
    "\n",
    "static_tokenizers_map = {name: AutoTokenizer.from_pretrained(name) for name in STATIC_TOKENIZERS}\n",
    "all_results = []\n",
    "\n",
    "for lang, config in DATASETS_CONFIG.items():\n",
    "    print(f\"\\n--- Processando lingua: {lang.upper()} ---\")\n",
    "\n",
    "    # Passando o nome da configuracao (quando houver) para a funcao load_dataset\n",
    "    dataset_name = config[\"name\"]\n",
    "    config_name = config.get(\"config_name\")\n",
    "    dataset = load_dataset(dataset_name, name=config_name, split='train').shuffle(seed=42)\n",
    "\n",
    "    # Seguranca na amostragem: pegar no maximo NUM_SAMPLES, ou menos se o dataset for menor\n",
    "    num_to_sample = min(NUM_SAMPLES, len(dataset))\n",
    "    sampled_dataset = dataset.select(range(num_to_sample))\n",
    "\n",
    "    for sample in tqdm(sampled_dataset, desc=f\"Dataset {lang}\"):\n",
    "        text_content = sample[config[\"text_column\"]]\n",
    "\n",
    "        # Filtro para textos muito longos ou vazios\n",
    "        if not text_content or len(text_content.split()) > MAX_TEXT_LENGTH_WORDS:\n",
    "            continue\n",
    "\n",
    "        for preprocessor in preprocessors:\n",
    "            # Se for MWE e a lingua nao for PT, usa versao lowercase simples\n",
    "            if isinstance(preprocessor, MWEPreprocessor) and lang != \"pt\":\n",
    "                processed_text = text_content.lower()\n",
    "                strategy_name = \"Baseline_Lower\"\n",
    "            else:\n",
    "                processed_text = preprocessor.apply(text_content)\n",
    "                strategy_name = str(preprocessor)\n",
    "\n",
    "            # Analise Estatica\n",
    "            for name, tokenizer in static_tokenizers_map.items():\n",
    "                num_tokens, fertility = get_static_metrics(processed_text, tokenizer)\n",
    "                all_results.append({\n",
    "                    \"language\": lang, \"strategy\": strategy_name, \"model_or_tokenizer\": name,\n",
    "                    \"metric_type\": \"static\", \"num_tokens\": num_tokens, \"fertility\": fertility, \"generation_time\": None\n",
    "                })\n",
    "\n",
    "            # Analise Dinamica\n",
    "            generation_time = get_dynamic_metrics(processed_text, dynamic_tokenizer, dynamic_model)\n",
    "            all_results.append({\n",
    "                \"language\": lang, \"strategy\": strategy_name, \"model_or_tokenizer\": DYNAMIC_MODEL,\n",
    "                \"metric_type\": \"dynamic\", \"num_tokens\": None, \"fertility\": None, \"generation_time\": generation_time\n",
    "            })\n",
    "\n",
    "# --- 6. SALVANDO OS RESULTADOS ---\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.to_csv(\"experiment_results.csv\", index=False)\n",
    "print(\"\\nExperimento concluido! Resultados salvos em 'experiment_results.csv'\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download('experiment_results.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
