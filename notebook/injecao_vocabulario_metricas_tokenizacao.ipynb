{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 0: Instalação\n",
    "!pip install --upgrade datasets transformers accelerate bitsandbytes gensim\n",
    "\n",
    "# Célula Principal: - Pipeline com Injeção de Vocabulário\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# --- 1. CONFIGURACOES ---\n",
    "NUM_SAMPLES = 500\n",
    "MAX_TEXT_LENGTH_WORDS = 4000\n",
    "PMI_CONFIG = {\n",
    "    \"min_count\": 5,\n",
    "    \"threshold\": 10.0\n",
    "}\n",
    "\n",
    "DATASETS_CONFIG = {\n",
    "    \"pt\": {\"name\": \"TucanoBR/GigaVerbo-Text-Filter\", \"config_name\": \"default\", \"text_column\": \"text\"},\n",
    "    \"en\": {\"name\": \"ag_news\", \"text_column\": \"text\"},\n",
    "}\n",
    "\n",
    "# Vamos carregar os tokenizadores estáticos agora para poder modificá-los depois\n",
    "STATIC_TOKENIZERS_NAMES = [\"xlm-roberta-base\", \"google/mt5-base\"]\n",
    "static_tokenizers_map = {name: AutoTokenizer.from_pretrained(name) for name in STATIC_TOKENIZERS_NAMES}\n",
    "\n",
    "DYNAMIC_MODEL_NAME = \"google/gemma-2b-it\"\n",
    "\n",
    "# --- 2. PRE-PROCESSADORES ---\n",
    "class BasePreprocessor:\n",
    "    def apply(self, text: str) -> str:\n",
    "        raise NotImplementedError\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "class BaselinePreprocessor(BasePreprocessor):\n",
    "    def apply(self, text: str) -> str:\n",
    "        return text\n",
    "\n",
    "class RegexPreprocessor(BasePreprocessor):\n",
    "    def apply(self, text: str) -> str:\n",
    "        processed_text = text.lower()\n",
    "        processed_text = re.sub(r'([.,!?;])', r' \\1 ', processed_text)\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "        return processed_text\n",
    "\n",
    "class LearnedMWEPreprocessor(BasePreprocessor):\n",
    "    def __init__(self, phraser_model):\n",
    "        self.phraser = phraser_model\n",
    "    def apply(self, text: str) -> str:\n",
    "        tokens = text.lower().split()\n",
    "        new_tokens = self.phraser[tokens]\n",
    "        return \" \".join(new_tokens)\n",
    "\n",
    "# --- 3. MODELO DINÂMICO ---\n",
    "print(\"Carregando modelo dinâmico...\")\n",
    "dynamic_tokenizer = AutoTokenizer.from_pretrained(DYNAMIC_MODEL_NAME)\n",
    "dynamic_model = AutoModelForCausalLM.from_pretrained(\n",
    "    DYNAMIC_MODEL_NAME, device_map=\"auto\", torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# --- 4. FUNÇÕES ---\n",
    "def train_and_inject_vocab(dataset, text_column, lang_code, tokenizers_list, dyn_model):\n",
    "    \"\"\"\n",
    "    1. Treina o PMI.\n",
    "    2. Extrai os MWEs.\n",
    "    3. ADICIONA AO VOCABULÁRIO dos tokenizadores.\n",
    "    \"\"\"\n",
    "    print(f\"--- Treinando PMI para {lang_code.upper()} ---\")\n",
    "    stream = (row[text_column].lower().split() for row in dataset)\n",
    "    phrases = Phrases(stream, min_count=PMI_CONFIG[\"min_count\"], threshold=PMI_CONFIG[\"threshold\"])\n",
    "    bigram_phraser = Phraser(phrases)\n",
    "    mwe_list = list(phrases.export_phrases().keys())\n",
    "    new_tokens = [mwe for mwe in mwe_list]\n",
    "\n",
    "    print(f\"MWEs detectados: {len(new_tokens)}. Ex: {new_tokens[:5]}\")\n",
    "\n",
    "    if len(new_tokens) > 0:\n",
    "        print(f\"Injetando {len(new_tokens)} novos tokens nos tokenizadores...\")\n",
    "\n",
    "        # 1. Atualiza Tokenizadores Estáticos\n",
    "        for name, tok in tokenizers_list.items():\n",
    "            num_added = tok.add_tokens(new_tokens)\n",
    "            print(f\"  -> {name}: {num_added} tokens adicionados.\")\n",
    "\n",
    "        # 2. Atualiza Tokenizador Dinâmico e o Modelo\n",
    "        num_added_dyn = dynamic_tokenizer.add_tokens(new_tokens)\n",
    "        if num_added_dyn > 0:\n",
    "            dyn_model.resize_token_embeddings(len(dynamic_tokenizer))\n",
    "            print(f\"  -> Modelo Dinâmico redimensionado com sucesso!\")\n",
    "\n",
    "    return bigram_phraser\n",
    "\n",
    "def get_static_metrics(text: str, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    num_tokens = len(tokens)\n",
    "    num_words = len(text.split())\n",
    "    fertility = num_tokens / num_words if num_words > 0 else 0\n",
    "    return num_tokens, fertility\n",
    "\n",
    "def get_dynamic_metrics(text: str, tokenizer, model):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# --- 5. EXECUÇÃO ---\n",
    "all_results = []\n",
    "\n",
    "for lang, config in DATASETS_CONFIG.items():\n",
    "    print(f\"\\n================ LÍNGUA: {lang.upper()} ================\")\n",
    "\n",
    "    # Recarrega dataset\n",
    "    dataset_name = config[\"name\"]\n",
    "    config_name = config.get(\"config_name\")\n",
    "    full_dataset = load_dataset(dataset_name, name=config_name, split='train').shuffle(seed=42)\n",
    "\n",
    "    # Treino (Subset)\n",
    "    train_subset = full_dataset.select(range(min(10000, len(full_dataset))))\n",
    "\n",
    "    # Treina E injeta no vocabulário\n",
    "    mwe_model = train_and_inject_vocab(\n",
    "        train_subset, config[\"text_column\"], lang,\n",
    "        static_tokenizers_map, dynamic_model\n",
    "    )\n",
    "\n",
    "    preprocessors = [\n",
    "        BaselinePreprocessor(),\n",
    "        RegexPreprocessor(),\n",
    "        LearnedMWEPreprocessor(phraser_model=mwe_model) # Vocabulário preparado\n",
    "    ]\n",
    "\n",
    "    # Amostragem para Teste\n",
    "    test_subset = full_dataset.select(range(min(NUM_SAMPLES, len(full_dataset))))\n",
    "\n",
    "    for sample in tqdm(test_subset, desc=f\"Medindo {lang}\"):\n",
    "        text_content = sample[config[\"text_column\"]]\n",
    "        if not text_content or len(text_content.split()) > MAX_TEXT_LENGTH_WORDS: continue\n",
    "\n",
    "        for preprocessor in preprocessors:\n",
    "            processed_text = preprocessor.apply(text_content)\n",
    "\n",
    "            if isinstance(preprocessor, LearnedMWEPreprocessor):\n",
    "                strategy_name = \"Automated_MWE_VocabInjection\" # Nome novo!\n",
    "            else:\n",
    "                strategy_name = str(preprocessor)\n",
    "\n",
    "            # Estático\n",
    "            for name, tokenizer in static_tokenizers_map.items():\n",
    "                num_tokens, fertility = get_static_metrics(processed_text, tokenizer)\n",
    "                all_results.append({\n",
    "                    \"language\": lang,\n",
    "                    \"strategy\": strategy_name,\n",
    "                    \"model_or_tokenizer\": name,\n",
    "                    \"metric_type\": \"static\",\n",
    "                    \"num_tokens\": num_tokens,\n",
    "                    \"fertility\": fertility,\n",
    "                    \"generation_time\": None\n",
    "                })\n",
    "\n",
    "            # Dinâmico\n",
    "            generation_time = get_dynamic_metrics(processed_text, dynamic_tokenizer, dynamic_model)\n",
    "            all_results.append({\n",
    "                \"language\": lang,\n",
    "                \"strategy\": strategy_name,\n",
    "                \"model_or_tokenizer\": DYNAMIC_MODEL_NAME,\n",
    "                \"metric_type\": \"dynamic\",\n",
    "                \"num_tokens\": None,\n",
    "                \"fertility\": None,\n",
    "                \"generation_time\": generation_time\n",
    "            })\n",
    "\n",
    "# Salvando\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.to_csv(\"vocab_injection_results.csv\", index=False)\n",
    "print(\"\\nSucesso! Vocabulário expandido e métricas coletadas.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
